{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e2c23a-dc5f-45b2-8e77-90f5fbcc00f0",
   "metadata": {},
   "source": [
    "### ARTIFICIAL NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d614110-8928-4ac0-9ea3-7eeb0ad9c4d1",
   "metadata": {},
   "source": [
    "#### \n",
    "Case Study: SONAR — Detecting Mines vs. Rocks\n",
    "\n",
    "1️.Business Objective\n",
    "\n",
    "Goal:\n",
    "To build an intelligent system that can automatically detect whether an underwater sonar signal is reflected from a metallic mine (potentially dangerous) or a harmless rock.\n",
    "\n",
    "This is vital for:\n",
    "\n",
    "1.Maritime safety: Prevent ships and submarines from colliding with mines.\n",
    "\n",
    "2.Naval defense: Identify and safely remove underwater mines.\n",
    "\n",
    "3.Resource exploration: Distinguish between useful metal structures and natural seabed objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e9e5d-867f-4620-b011-9601df96c5df",
   "metadata": {},
   "source": [
    "### \n",
    "2️.Problem Statement\n",
    "\n",
    "In underwater environments, sonar (sound navigation and ranging) is used to detect objects. However, raw sonar signals can be noisy and difficult for humans to interpret consistently.\n",
    "\n",
    "This dataset:\n",
    "\n",
    "•\tContains 208 sonar returns.\n",
    "o\t111 are from metal cylinders (mines).\n",
    "o\t97 are from rocks.\n",
    "•\tEach sonar return is represented by 60 numeric features, each measuring the energy of the signal in a frequency band.\n",
    "\n",
    "The problem:\n",
    "\n",
    "To train a Deep learning model that can learn the difference in signal patterns and classify new sonar signals as either Mine (M) or Rock (R) — accurately and reliably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01fe309-caeb-4c4c-9b8b-eee514c5ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922848b5-c664-4917-a48d-aa9ed87d277f",
   "metadata": {},
   "source": [
    "#### Dataset: \"sonardataset.csv\"\n",
    "\n",
    "Features (Inputs)\n",
    "•\tThere are 60 numerical variables, each representing the energy in a specific frequency band of the sonar signal.\n",
    "•\tIn the original dataset, they’re just unnamed columns V1, V2, ..., V60 — you can keep it clear and simple:\n",
    "\n",
    "⃣2Target (Output)\n",
    "•\tThe label is a single categorical variable indicating:\n",
    "o\t\"M\" for Mine\n",
    "o\t\"R\" for Rock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391c998d-2cc3-49b3-a613-2fe5ac1e1b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sonardataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be5fd95-095c-4e4b-a4ee-fc7f11d34829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>...</th>\n",
       "      <th>x_52</th>\n",
       "      <th>x_53</th>\n",
       "      <th>x_54</th>\n",
       "      <th>x_55</th>\n",
       "      <th>x_56</th>\n",
       "      <th>x_57</th>\n",
       "      <th>x_58</th>\n",
       "      <th>x_59</th>\n",
       "      <th>x_60</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x_1     x_2     x_3     x_4     x_5     x_6     x_7     x_8     x_9  \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "     x_10  ...    x_52    x_53    x_54    x_55    x_56    x_57    x_58  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "     x_59    x_60  Y  \n",
       "0  0.0090  0.0032  R  \n",
       "1  0.0052  0.0044  R  \n",
       "2  0.0095  0.0078  R  \n",
       "3  0.0040  0.0117  R  \n",
       "4  0.0107  0.0094  R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e846d6cd-c62f-4cc6-8e5d-6bd8491b4210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x_1     208 non-null    float64\n",
      " 1   x_2     208 non-null    float64\n",
      " 2   x_3     208 non-null    float64\n",
      " 3   x_4     208 non-null    float64\n",
      " 4   x_5     208 non-null    float64\n",
      " 5   x_6     208 non-null    float64\n",
      " 6   x_7     208 non-null    float64\n",
      " 7   x_8     208 non-null    float64\n",
      " 8   x_9     208 non-null    float64\n",
      " 9   x_10    208 non-null    float64\n",
      " 10  x_11    208 non-null    float64\n",
      " 11  x_12    208 non-null    float64\n",
      " 12  x_13    208 non-null    float64\n",
      " 13  x_14    208 non-null    float64\n",
      " 14  x_15    208 non-null    float64\n",
      " 15  x_16    208 non-null    float64\n",
      " 16  x_17    208 non-null    float64\n",
      " 17  x_18    208 non-null    float64\n",
      " 18  x_19    208 non-null    float64\n",
      " 19  x_20    208 non-null    float64\n",
      " 20  x_21    208 non-null    float64\n",
      " 21  x_22    208 non-null    float64\n",
      " 22  x_23    208 non-null    float64\n",
      " 23  x_24    208 non-null    float64\n",
      " 24  x_25    208 non-null    float64\n",
      " 25  x_26    208 non-null    float64\n",
      " 26  x_27    208 non-null    float64\n",
      " 27  x_28    208 non-null    float64\n",
      " 28  x_29    208 non-null    float64\n",
      " 29  x_30    208 non-null    float64\n",
      " 30  x_31    208 non-null    float64\n",
      " 31  x_32    208 non-null    float64\n",
      " 32  x_33    208 non-null    float64\n",
      " 33  x_34    208 non-null    float64\n",
      " 34  x_35    208 non-null    float64\n",
      " 35  x_36    208 non-null    float64\n",
      " 36  x_37    208 non-null    float64\n",
      " 37  x_38    208 non-null    float64\n",
      " 38  x_39    208 non-null    float64\n",
      " 39  x_40    208 non-null    float64\n",
      " 40  x_41    208 non-null    float64\n",
      " 41  x_42    208 non-null    float64\n",
      " 42  x_43    208 non-null    float64\n",
      " 43  x_44    208 non-null    float64\n",
      " 44  x_45    208 non-null    float64\n",
      " 45  x_46    208 non-null    float64\n",
      " 46  x_47    208 non-null    float64\n",
      " 47  x_48    208 non-null    float64\n",
      " 48  x_49    208 non-null    float64\n",
      " 49  x_50    208 non-null    float64\n",
      " 50  x_51    208 non-null    float64\n",
      " 51  x_52    208 non-null    float64\n",
      " 52  x_53    208 non-null    float64\n",
      " 53  x_54    208 non-null    float64\n",
      " 54  x_55    208 non-null    float64\n",
      " 55  x_56    208 non-null    float64\n",
      " 56  x_57    208 non-null    float64\n",
      " 57  x_58    208 non-null    float64\n",
      " 58  x_59    208 non-null    float64\n",
      " 59  x_60    208 non-null    float64\n",
      " 60  Y       208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9476d221-7a2f-42c8-b945-e3dd903ded5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n",
      "M    111\n",
      "R     97\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if df.shape[1] >= 2:\n",
    "    labels = df.iloc[:, -1]\n",
    "    print(labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a54837-c018-4a67-960f-7f4d78251dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 60)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric = df.select_dtypes(include=[np.number])\n",
    "numeric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7efdf1dc-5c04-4bce-8158-036af1b8201e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x_1</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.02280</td>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.1371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_2</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.03080</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_3</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.03430</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.3059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_4</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.04405</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.4264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_5</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_6</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.09215</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.3823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_7</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.10695</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.3729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_8</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_9</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.15225</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.6828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_10</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>0.18240</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.7106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_11</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.236013</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.22480</td>\n",
       "      <td>0.301650</td>\n",
       "      <td>0.7342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_12</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.250221</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.24905</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.7060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_13</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>0.26395</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.7131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_14</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.296568</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.28110</td>\n",
       "      <td>0.386175</td>\n",
       "      <td>0.9970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_15</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.320201</td>\n",
       "      <td>0.205427</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.164625</td>\n",
       "      <td>0.28170</td>\n",
       "      <td>0.452925</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_16</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.378487</td>\n",
       "      <td>0.232650</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.30470</td>\n",
       "      <td>0.535725</td>\n",
       "      <td>0.9988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_17</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.263677</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.30840</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_18</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.452318</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.242075</td>\n",
       "      <td>0.36830</td>\n",
       "      <td>0.679050</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_19</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.504812</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.299075</td>\n",
       "      <td>0.43495</td>\n",
       "      <td>0.731400</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_20</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.563047</td>\n",
       "      <td>0.262653</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.350625</td>\n",
       "      <td>0.54250</td>\n",
       "      <td>0.809325</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_21</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.609060</td>\n",
       "      <td>0.257818</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.399725</td>\n",
       "      <td>0.61770</td>\n",
       "      <td>0.816975</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_22</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.624275</td>\n",
       "      <td>0.255883</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.406925</td>\n",
       "      <td>0.66490</td>\n",
       "      <td>0.831975</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_23</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.646975</td>\n",
       "      <td>0.250175</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>0.450225</td>\n",
       "      <td>0.69970</td>\n",
       "      <td>0.848575</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_24</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.239116</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.540725</td>\n",
       "      <td>0.69850</td>\n",
       "      <td>0.872175</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_25</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.675424</td>\n",
       "      <td>0.244926</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.72110</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_26</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.544175</td>\n",
       "      <td>0.75450</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_27</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.702155</td>\n",
       "      <td>0.245657</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.74560</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_28</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.694024</td>\n",
       "      <td>0.237189</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.534775</td>\n",
       "      <td>0.73190</td>\n",
       "      <td>0.900275</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_29</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.642074</td>\n",
       "      <td>0.240250</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.68080</td>\n",
       "      <td>0.852125</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_30</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.220749</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.60715</td>\n",
       "      <td>0.735175</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_31</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.504475</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.345550</td>\n",
       "      <td>0.49035</td>\n",
       "      <td>0.641950</td>\n",
       "      <td>0.9657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_32</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.213237</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.42960</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.9306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_33</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.417220</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.257875</td>\n",
       "      <td>0.39120</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_34</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.403233</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.217575</td>\n",
       "      <td>0.35105</td>\n",
       "      <td>0.596125</td>\n",
       "      <td>0.9647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_35</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.392571</td>\n",
       "      <td>0.259132</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.179375</td>\n",
       "      <td>0.31275</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_36</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.384848</td>\n",
       "      <td>0.264121</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.154350</td>\n",
       "      <td>0.32115</td>\n",
       "      <td>0.556525</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_37</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.363807</td>\n",
       "      <td>0.239912</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.30630</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.9497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_38</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.339657</td>\n",
       "      <td>0.212973</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.174275</td>\n",
       "      <td>0.31270</td>\n",
       "      <td>0.440550</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_39</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.199075</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.173975</td>\n",
       "      <td>0.28350</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.9857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_40</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.311207</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>0.27805</td>\n",
       "      <td>0.424350</td>\n",
       "      <td>0.9297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_41</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.289252</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.25950</td>\n",
       "      <td>0.387525</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_42</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.278293</td>\n",
       "      <td>0.168728</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.24510</td>\n",
       "      <td>0.384250</td>\n",
       "      <td>0.8246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_43</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.246542</td>\n",
       "      <td>0.138993</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.22255</td>\n",
       "      <td>0.324525</td>\n",
       "      <td>0.7733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_44</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.214075</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.126875</td>\n",
       "      <td>0.17770</td>\n",
       "      <td>0.271750</td>\n",
       "      <td>0.7762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_45</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.197232</td>\n",
       "      <td>0.151628</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.094475</td>\n",
       "      <td>0.14800</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.7034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_46</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.160631</td>\n",
       "      <td>0.133938</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.068550</td>\n",
       "      <td>0.12135</td>\n",
       "      <td>0.200375</td>\n",
       "      <td>0.7292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_47</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.086953</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.10165</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>0.5522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_48</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.045125</td>\n",
       "      <td>0.07810</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.3339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_49</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.051929</td>\n",
       "      <td>0.035954</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.04470</td>\n",
       "      <td>0.068525</td>\n",
       "      <td>0.1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_50</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.01790</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.0825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_51</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.01390</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_52</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.01140</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.0709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_53</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.00955</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.0390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_54</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.00930</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.0352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_55</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.0447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_56</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.00685</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.0394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_57</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.00595</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.0355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_58</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.00580</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.0440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_59</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.00640</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_60</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.006507</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.00530</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>0.0439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean       std     min       25%      50%       75%     max\n",
       "x_1   208.0  0.029164  0.022991  0.0015  0.013350  0.02280  0.035550  0.1371\n",
       "x_2   208.0  0.038437  0.032960  0.0006  0.016450  0.03080  0.047950  0.2339\n",
       "x_3   208.0  0.043832  0.038428  0.0015  0.018950  0.03430  0.057950  0.3059\n",
       "x_4   208.0  0.053892  0.046528  0.0058  0.024375  0.04405  0.064500  0.4264\n",
       "x_5   208.0  0.075202  0.055552  0.0067  0.038050  0.06250  0.100275  0.4010\n",
       "x_6   208.0  0.104570  0.059105  0.0102  0.067025  0.09215  0.134125  0.3823\n",
       "x_7   208.0  0.121747  0.061788  0.0033  0.080900  0.10695  0.154000  0.3729\n",
       "x_8   208.0  0.134799  0.085152  0.0055  0.080425  0.11210  0.169600  0.4590\n",
       "x_9   208.0  0.178003  0.118387  0.0075  0.097025  0.15225  0.233425  0.6828\n",
       "x_10  208.0  0.208259  0.134416  0.0113  0.111275  0.18240  0.268700  0.7106\n",
       "x_11  208.0  0.236013  0.132705  0.0289  0.129250  0.22480  0.301650  0.7342\n",
       "x_12  208.0  0.250221  0.140072  0.0236  0.133475  0.24905  0.331250  0.7060\n",
       "x_13  208.0  0.273305  0.140962  0.0184  0.166125  0.26395  0.351250  0.7131\n",
       "x_14  208.0  0.296568  0.164474  0.0273  0.175175  0.28110  0.386175  0.9970\n",
       "x_15  208.0  0.320201  0.205427  0.0031  0.164625  0.28170  0.452925  1.0000\n",
       "x_16  208.0  0.378487  0.232650  0.0162  0.196300  0.30470  0.535725  0.9988\n",
       "x_17  208.0  0.415983  0.263677  0.0349  0.205850  0.30840  0.659425  1.0000\n",
       "x_18  208.0  0.452318  0.261529  0.0375  0.242075  0.36830  0.679050  1.0000\n",
       "x_19  208.0  0.504812  0.257988  0.0494  0.299075  0.43495  0.731400  1.0000\n",
       "x_20  208.0  0.563047  0.262653  0.0656  0.350625  0.54250  0.809325  1.0000\n",
       "x_21  208.0  0.609060  0.257818  0.0512  0.399725  0.61770  0.816975  1.0000\n",
       "x_22  208.0  0.624275  0.255883  0.0219  0.406925  0.66490  0.831975  1.0000\n",
       "x_23  208.0  0.646975  0.250175  0.0563  0.450225  0.69970  0.848575  1.0000\n",
       "x_24  208.0  0.672654  0.239116  0.0239  0.540725  0.69850  0.872175  1.0000\n",
       "x_25  208.0  0.675424  0.244926  0.0240  0.525800  0.72110  0.873725  1.0000\n",
       "x_26  208.0  0.699866  0.237228  0.0921  0.544175  0.75450  0.893800  1.0000\n",
       "x_27  208.0  0.702155  0.245657  0.0481  0.531900  0.74560  0.917100  1.0000\n",
       "x_28  208.0  0.694024  0.237189  0.0284  0.534775  0.73190  0.900275  1.0000\n",
       "x_29  208.0  0.642074  0.240250  0.0144  0.463700  0.68080  0.852125  1.0000\n",
       "x_30  208.0  0.580928  0.220749  0.0613  0.411400  0.60715  0.735175  1.0000\n",
       "x_31  208.0  0.504475  0.213992  0.0482  0.345550  0.49035  0.641950  0.9657\n",
       "x_32  208.0  0.439040  0.213237  0.0404  0.281400  0.42960  0.580300  0.9306\n",
       "x_33  208.0  0.417220  0.206513  0.0477  0.257875  0.39120  0.556125  1.0000\n",
       "x_34  208.0  0.403233  0.231242  0.0212  0.217575  0.35105  0.596125  0.9647\n",
       "x_35  208.0  0.392571  0.259132  0.0223  0.179375  0.31275  0.593350  1.0000\n",
       "x_36  208.0  0.384848  0.264121  0.0080  0.154350  0.32115  0.556525  1.0000\n",
       "x_37  208.0  0.363807  0.239912  0.0351  0.160100  0.30630  0.518900  0.9497\n",
       "x_38  208.0  0.339657  0.212973  0.0383  0.174275  0.31270  0.440550  1.0000\n",
       "x_39  208.0  0.325800  0.199075  0.0371  0.173975  0.28350  0.434900  0.9857\n",
       "x_40  208.0  0.311207  0.178662  0.0117  0.186450  0.27805  0.424350  0.9297\n",
       "x_41  208.0  0.289252  0.171111  0.0360  0.163100  0.25950  0.387525  0.8995\n",
       "x_42  208.0  0.278293  0.168728  0.0056  0.158900  0.24510  0.384250  0.8246\n",
       "x_43  208.0  0.246542  0.138993  0.0000  0.155200  0.22255  0.324525  0.7733\n",
       "x_44  208.0  0.214075  0.133291  0.0000  0.126875  0.17770  0.271750  0.7762\n",
       "x_45  208.0  0.197232  0.151628  0.0000  0.094475  0.14800  0.231550  0.7034\n",
       "x_46  208.0  0.160631  0.133938  0.0000  0.068550  0.12135  0.200375  0.7292\n",
       "x_47  208.0  0.122453  0.086953  0.0000  0.064250  0.10165  0.154425  0.5522\n",
       "x_48  208.0  0.091424  0.062417  0.0000  0.045125  0.07810  0.120100  0.3339\n",
       "x_49  208.0  0.051929  0.035954  0.0000  0.026350  0.04470  0.068525  0.1981\n",
       "x_50  208.0  0.020424  0.013665  0.0000  0.011550  0.01790  0.025275  0.0825\n",
       "x_51  208.0  0.016069  0.012008  0.0000  0.008425  0.01390  0.020825  0.1004\n",
       "x_52  208.0  0.013420  0.009634  0.0008  0.007275  0.01140  0.016725  0.0709\n",
       "x_53  208.0  0.010709  0.007060  0.0005  0.005075  0.00955  0.014900  0.0390\n",
       "x_54  208.0  0.010941  0.007301  0.0010  0.005375  0.00930  0.014500  0.0352\n",
       "x_55  208.0  0.009290  0.007088  0.0006  0.004150  0.00750  0.012100  0.0447\n",
       "x_56  208.0  0.008222  0.005736  0.0004  0.004400  0.00685  0.010575  0.0394\n",
       "x_57  208.0  0.007820  0.005785  0.0003  0.003700  0.00595  0.010425  0.0355\n",
       "x_58  208.0  0.007949  0.006470  0.0003  0.003600  0.00580  0.010350  0.0440\n",
       "x_59  208.0  0.007941  0.006181  0.0001  0.003675  0.00640  0.010325  0.0364\n",
       "x_60  208.0  0.006507  0.005031  0.0006  0.003100  0.00530  0.008525  0.0439"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d680a27a-fa54-4019-84f2-c147de5623f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217083d7-e9e1-4dd4-9bfe-4122889b7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values.astype(float)  \n",
    "y_raw = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218d6e6-ef3d-4f71-bfd5-46c8f0125ee1",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "1. Data Exploration and Preprocessing\n",
    "   \n",
    "●\tBegin by loading and exploring the \"Alphabets_data.csv\" dataset. Summarize its key features such as the number of samples, features, and classes.\n",
    "●\tExecute necessary data preprocessing steps including data normalization, managing missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950a7af6-19c5-473e-b1f1-12d93d4d9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8064f0f2-91d4-47b4-a237-8c90f2eb18d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M': np.int64(0), 'R': np.int64(1)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(le.classes_, le.transform(le.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee8c407-a639-4c01-bd1e-0e0af8c5dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce1b174d-fe1a-47fc-996e-92e2dc9e5ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((166, 60), (42, 60), (166,), (42,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259966cc-4c80-46d1-b50e-032db9fb88d4",
   "metadata": {},
   "source": [
    "#### 2. Model Implementation\n",
    "\n",
    "●\tConstruct a basic ANN model using your chosen high-level neural network library. Ensure your model includes at least one hidden layer.\n",
    "●\tDivide the dataset into training and test sets.\n",
    "●\tTrain your model on the training set and then use it to make predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c3971e3-9c96-4dc4-9776-545f8db1c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, hidden_layers=(32,), activation='relu', dropout=0.0, lr=1e-3, optimizer_name='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers[0], input_dim=input_dim, activation=activation))\n",
    "    if dropout>0:\n",
    "        model.add(Dropout(dropout))\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        if dropout>0:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    if optimizer_name == 'adam':\n",
    "        opt = Adam(learning_rate=lr)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaf2bae4-9b4d-4405-828b-795ff514d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model: {'hidden_layers': (32,), 'activation': 'relu', 'dropout': 0.0, 'lr': 0.001, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 16}\n",
      "Epoch 1/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5034 - loss: 0.7300 - val_accuracy: 0.6471 - val_loss: 0.5926\n",
      "Epoch 2/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5772 - loss: 0.6283 - val_accuracy: 0.7059 - val_loss: 0.5254\n",
      "Epoch 3/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6779 - loss: 0.5585 - val_accuracy: 0.8235 - val_loss: 0.4772\n",
      "Epoch 4/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7651 - loss: 0.5088 - val_accuracy: 0.8824 - val_loss: 0.4414\n",
      "Epoch 5/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7785 - loss: 0.4685 - val_accuracy: 0.9412 - val_loss: 0.4149\n",
      "Epoch 6/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8121 - loss: 0.4357 - val_accuracy: 0.9412 - val_loss: 0.3927\n",
      "Epoch 7/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8255 - loss: 0.4070 - val_accuracy: 0.9412 - val_loss: 0.3747\n",
      "Epoch 8/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8456 - loss: 0.3833 - val_accuracy: 0.9412 - val_loss: 0.3620\n",
      "Epoch 9/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8725 - loss: 0.3619 - val_accuracy: 0.9412 - val_loss: 0.3439\n",
      "Epoch 10/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8792 - loss: 0.3417 - val_accuracy: 0.9412 - val_loss: 0.3302\n",
      "Epoch 11/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8859 - loss: 0.3242 - val_accuracy: 1.0000 - val_loss: 0.3198\n",
      "Epoch 12/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8993 - loss: 0.3078 - val_accuracy: 1.0000 - val_loss: 0.3067\n",
      "Epoch 13/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8926 - loss: 0.2932 - val_accuracy: 0.9412 - val_loss: 0.2946\n",
      "Epoch 14/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9128 - loss: 0.2782 - val_accuracy: 0.9412 - val_loss: 0.2839\n",
      "Epoch 15/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9396 - loss: 0.2653 - val_accuracy: 0.9412 - val_loss: 0.2781\n",
      "Epoch 16/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9463 - loss: 0.2526 - val_accuracy: 0.9412 - val_loss: 0.2675\n",
      "Epoch 17/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9597 - loss: 0.2404 - val_accuracy: 0.9412 - val_loss: 0.2615\n",
      "Epoch 18/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9597 - loss: 0.2306 - val_accuracy: 0.9412 - val_loss: 0.2538\n",
      "Epoch 19/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9732 - loss: 0.2192 - val_accuracy: 0.9412 - val_loss: 0.2476\n",
      "Epoch 20/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9799 - loss: 0.2100 - val_accuracy: 0.9412 - val_loss: 0.2402\n",
      "Epoch 21/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9799 - loss: 0.2007 - val_accuracy: 0.8824 - val_loss: 0.2351\n",
      "Epoch 22/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9799 - loss: 0.1914 - val_accuracy: 0.9412 - val_loss: 0.2289\n",
      "Epoch 23/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9866 - loss: 0.1830 - val_accuracy: 0.9412 - val_loss: 0.2246\n",
      "Epoch 24/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9933 - loss: 0.1758 - val_accuracy: 0.9412 - val_loss: 0.2210\n",
      "Epoch 25/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9933 - loss: 0.1677 - val_accuracy: 0.9412 - val_loss: 0.2200\n",
      "Epoch 26/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9933 - loss: 0.1614 - val_accuracy: 0.9412 - val_loss: 0.2200\n",
      "Epoch 27/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9933 - loss: 0.1544 - val_accuracy: 0.9412 - val_loss: 0.2154\n",
      "Epoch 28/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1482 - val_accuracy: 0.9412 - val_loss: 0.2099\n",
      "Epoch 29/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9933 - loss: 0.1421 - val_accuracy: 0.9412 - val_loss: 0.2086\n",
      "Epoch 30/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1356 - val_accuracy: 0.9412 - val_loss: 0.2071\n",
      "Epoch 31/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1303 - val_accuracy: 0.9412 - val_loss: 0.2053\n",
      "Epoch 32/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9933 - loss: 0.1255 - val_accuracy: 0.9412 - val_loss: 0.2031\n",
      "Epoch 33/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1207 - val_accuracy: 0.9412 - val_loss: 0.2031\n",
      "Epoch 34/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9933 - loss: 0.1155 - val_accuracy: 0.9412 - val_loss: 0.2004\n",
      "Epoch 35/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1117 - val_accuracy: 0.9412 - val_loss: 0.1981\n",
      "Epoch 36/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.1063 - val_accuracy: 0.9412 - val_loss: 0.2022\n",
      "Epoch 37/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1027 - val_accuracy: 0.9412 - val_loss: 0.2032\n",
      "Epoch 38/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0985 - val_accuracy: 0.9412 - val_loss: 0.2021\n",
      "Epoch 39/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0948 - val_accuracy: 0.9412 - val_loss: 0.1992\n",
      "Epoch 40/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0910 - val_accuracy: 0.9412 - val_loss: 0.1992\n",
      "Epoch 41/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0875 - val_accuracy: 0.9412 - val_loss: 0.1978\n",
      "Epoch 42/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0851 - val_accuracy: 0.9412 - val_loss: 0.2000\n",
      "Epoch 43/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0813 - val_accuracy: 0.9412 - val_loss: 0.1993\n",
      "Epoch 44/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0781 - val_accuracy: 0.9412 - val_loss: 0.1991\n",
      "Epoch 45/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0755 - val_accuracy: 0.9412 - val_loss: 0.1997\n",
      "Epoch 46/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0725 - val_accuracy: 0.9412 - val_loss: 0.1988\n",
      "Epoch 47/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0702 - val_accuracy: 0.9412 - val_loss: 0.1994\n",
      "Epoch 48/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0673 - val_accuracy: 0.9412 - val_loss: 0.1991\n",
      "Epoch 49/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0649 - val_accuracy: 0.9412 - val_loss: 0.2010\n",
      "Epoch 50/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0627 - val_accuracy: 0.9412 - val_loss: 0.1999\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "base_params = dict(hidden_layers=(32,), activation='relu', dropout=0.0, lr=1e-3, optimizer_name='adam', epochs=50, batch_size=16)\n",
    "print(\"Training baseline model:\", base_params)\n",
    "base_model = build_model(input_dim, hidden_layers=base_params['hidden_layers'],\n",
    "                         activation=base_params['activation'], dropout=base_params['dropout'],\n",
    "                         lr=base_params['lr'], optimizer_name=base_params['optimizer_name'])\n",
    "hist_base = base_model.fit(X_train, y_train, validation_split=0.1, epochs=base_params['epochs'],\n",
    "                           batch_size=base_params['batch_size'], verbose=1)\n",
    "y_prob = base_model.predict(X_test).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5584dcd-f2a8-4c28-8c9f-8fc0405632f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccaf953a-60d7-4bb0-be5e-d1aaa5a82182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8823529411764706"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc8bb20a-75c7-46ed-8d5e-a8e983540e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7da84c91-5181-4353-8a57-bcc1c1891e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8108108108108109"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "490d316c-3854-4adf-905c-6fead9b266ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           M       0.80      0.91      0.85        22\\n           R       0.88      0.75      0.81        20\\n\\n    accuracy                           0.83        42\\n   macro avg       0.84      0.83      0.83        42\\nweighted avg       0.84      0.83      0.83        42\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred, target_names=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "173a0a9e-3e1f-4348-8877-1f42fa756c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  2],\n",
       "       [ 5, 15]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb30e5-b3d3-4d6f-92c4-1f4fd21a3607",
   "metadata": {},
   "source": [
    "#### 3. Hyperparameter Tuning\n",
    "\n",
    "●\tModify various hyperparameters, such as the number of hidden layers, neurons per hidden layer, activation functions, and learning rate, to observe their impact on model performance.\n",
    "●\tAdopt a structured approach like grid search or random search for hyperparameter tuning, documenting your methodology thoroughly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afc5b27a-cada-48ae-938b-93428a8504cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_layers': [(16,), (32,), (64,), (32,32), (64,32)],\n",
    "    'activation': ['relu','tanh'],\n",
    "    'dropout': [0.0, 0.1, 0.2],\n",
    "    'lr': [1e-2, 1e-3, 5e-4],\n",
    "    'optimizer_name': ['adam','sgd'],\n",
    "    'epochs': [50, 100],\n",
    "    'batch_size': [8, 16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17cd41d3-b06c-453b-a471-7f7abef88195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "all_combinations = list(product(*(param_grid[k] for k in param_grid)))\n",
    "len(all_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9be784d-91cc-4895-9930-7d03e6818d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n_iter = 12\n",
    "random.seed(42)\n",
    "sampled = random.sample(all_combinations, min(n_iter, len(all_combinations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f27cebcd-e63c-4bc7-b489-2586a67473d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64dac29a-268f-44d5-a97c-002d1d2f4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8489b610-7a7a-428d-b5a4-681eefbd3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/12] Testing params: {'hidden_layers': (64, 32), 'activation': 'tanh', 'dropout': 0.0, 'lr': 0.01, 'optimizer_name': 'sgd', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DF49023560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DF49023560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      " -> mean CV F1: 0.8371406371406371\n",
      "\n",
      "[2/12] Testing params: {'hidden_layers': (16,), 'activation': 'tanh', 'dropout': 0.1, 'lr': 0.0005, 'optimizer_name': 'adam', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      " -> mean CV F1: 0.7677314945116802\n",
      "\n",
      "[3/12] Testing params: {'hidden_layers': (16,), 'activation': 'relu', 'dropout': 0.1, 'lr': 0.01, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 16}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      " -> mean CV F1: 0.7738359201773837\n",
      "\n",
      "[4/12] Testing params: {'hidden_layers': (32,), 'activation': 'tanh', 'dropout': 0.2, 'lr': 0.0005, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 16}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      " -> mean CV F1: 0.7822871131316664\n",
      "\n",
      "[5/12] Testing params: {'hidden_layers': (32,), 'activation': 'tanh', 'dropout': 0.1, 'lr': 0.001, 'optimizer_name': 'adam', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      " -> mean CV F1: 0.755060959512324\n",
      "\n",
      "[6/12] Testing params: {'hidden_layers': (32,), 'activation': 'tanh', 'dropout': 0.0, 'lr': 0.001, 'optimizer_name': 'sgd', 'epochs': 50, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      " -> mean CV F1: 0.6687653562653563\n",
      "\n",
      "[7/12] Testing params: {'hidden_layers': (16,), 'activation': 'tanh', 'dropout': 0.2, 'lr': 0.0005, 'optimizer_name': 'sgd', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      " -> mean CV F1: 0.702517651430695\n",
      "\n",
      "[8/12] Testing params: {'hidden_layers': (16,), 'activation': 'tanh', 'dropout': 0.1, 'lr': 0.001, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      " -> mean CV F1: 0.7894744483159117\n",
      "\n",
      "[9/12] Testing params: {'hidden_layers': (64, 32), 'activation': 'tanh', 'dropout': 0.1, 'lr': 0.0005, 'optimizer_name': 'sgd', 'epochs': 50, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      " -> mean CV F1: 0.6840853606593822\n",
      "\n",
      "[10/12] Testing params: {'hidden_layers': (32, 32), 'activation': 'tanh', 'dropout': 0.2, 'lr': 0.01, 'optimizer_name': 'sgd', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      " -> mean CV F1: 0.7636922697898307\n",
      "\n",
      "[11/12] Testing params: {'hidden_layers': (16,), 'activation': 'tanh', 'dropout': 0.0, 'lr': 0.0005, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 16}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      " -> mean CV F1: 0.7269343089117808\n",
      "\n",
      "[12/12] Testing params: {'hidden_layers': (64, 32), 'activation': 'relu', 'dropout': 0.1, 'lr': 0.01, 'optimizer_name': 'sgd', 'epochs': 50, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      " -> mean CV F1: 0.7573593073593073\n",
      "\n",
      "Random search done in 7.11 minutes.\n",
      "Best CV F1: 0.8371406371406371\n",
      "Best params: {'hidden_layers': (64, 32), 'activation': 'tanh', 'dropout': 0.0, 'lr': 0.01, 'optimizer_name': 'sgd', 'epochs': 100, 'batch_size': 8}\n"
     ]
    }
   ],
   "source": [
    "start_all = time.time()\n",
    "best_score = -1.0\n",
    "best_res = None\n",
    "results = []\n",
    "for idx, combo in enumerate(sampled, 1):\n",
    "    params = dict(zip(param_grid.keys(), combo))\n",
    "    print(f\"\\n[{idx}/{len(sampled)}] Testing params: {params}\")\n",
    "    cv_scores = []\n",
    "    # cross-validation loop\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        model = build_model(input_dim, hidden_layers=params['hidden_layers'],\n",
    "                            activation=params['activation'], dropout=params['dropout'],\n",
    "                            lr=params['lr'], optimizer_name=params['optimizer_name'])\n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0)\n",
    "        yv = (model.predict(X_val).ravel() >= 0.5).astype(int)\n",
    "        cv_scores.append(f1_score(y_val, yv, zero_division=0))\n",
    "    mean_cv = np.mean(cv_scores)\n",
    "    results.append((params, mean_cv))\n",
    "    print(\" -> mean CV F1:\", mean_cv)\n",
    "    if mean_cv > best_score:\n",
    "        best_score = mean_cv\n",
    "        best_res = (params, mean_cv)\n",
    "end_all = time.time()\n",
    "print(f\"\\nRandom search done in {(end_all - start_all)/60:.2f} minutes.\")\n",
    "print(\"Best CV F1:\", best_score)\n",
    "print(\"Best params:\", best_res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea60378-cf48-4452-bde3-d5eeaca981f2",
   "metadata": {},
   "source": [
    "### 4. Evaluation\n",
    "●\tEmploy suitable metrics such as accuracy, precision, recall, and F1-score to evaluate your model's performance.\n",
    "●\tDiscuss the performance differences between the model with default hyperparameters and the tuned model, emphasizing the effects of hyperparameter tuning.\n",
    "\n",
    "Evaluation Criteria\n",
    "\n",
    "●\tAccuracy and completeness of the implementation.\n",
    "●\tProficiency in data preprocessing and model development.\n",
    "●\tSystematic approach and thoroughness in hyperparameter tuning.\n",
    "●\tDepth of evaluation and discussion.\n",
    "●\tOverall quality of the report.\n",
    "\n",
    "Additional Resources\n",
    "\n",
    "●\tTensorFlow Documentation\n",
    "●\tKeras Documentation\n",
    "We wish you the best of luck with this assignment. Enjoy exploring the fascinating world of neural networks and the power of hyperparameter tuning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f690131-2071-4553-b863-6daee9db8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_res[0]\n",
    "final_model = build_model(input_dim, hidden_layers=best_params['hidden_layers'],\n",
    "                          activation=best_params['activation'], dropout=best_params['dropout'],\n",
    "                          lr=best_params['lr'], optimizer_name=best_params['optimizer_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa277f7f-f34a-42de-b61a-6c31a312e44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layers': (64, 32),\n",
       " 'activation': 'tanh',\n",
       " 'dropout': 0.0,\n",
       " 'lr': 0.01,\n",
       " 'optimizer_name': 'sgd',\n",
       " 'epochs': 100,\n",
       " 'batch_size': 8}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f71846f4-6180-4390-818e-3afa4c5601e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5302 - loss: 0.6931 - val_accuracy: 0.6471 - val_loss: 0.5971\n",
      "Epoch 2/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7114 - loss: 0.5880 - val_accuracy: 0.6471 - val_loss: 0.5405\n",
      "Epoch 3/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7584 - loss: 0.5320 - val_accuracy: 0.7647 - val_loss: 0.5070\n",
      "Epoch 4/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7785 - loss: 0.4973 - val_accuracy: 0.7059 - val_loss: 0.4826\n",
      "Epoch 5/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7852 - loss: 0.4724 - val_accuracy: 0.7059 - val_loss: 0.4663\n",
      "Epoch 6/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7919 - loss: 0.4532 - val_accuracy: 0.7059 - val_loss: 0.4529\n",
      "Epoch 7/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8054 - loss: 0.4361 - val_accuracy: 0.7059 - val_loss: 0.4400\n",
      "Epoch 8/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8054 - loss: 0.4216 - val_accuracy: 0.7059 - val_loss: 0.4298\n",
      "Epoch 9/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8188 - loss: 0.4089 - val_accuracy: 0.7059 - val_loss: 0.4222\n",
      "Epoch 10/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8188 - loss: 0.3952 - val_accuracy: 0.8235 - val_loss: 0.4175\n",
      "Epoch 11/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8121 - loss: 0.3837 - val_accuracy: 0.8824 - val_loss: 0.4099\n",
      "Epoch 12/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8322 - loss: 0.3728 - val_accuracy: 0.8824 - val_loss: 0.4040\n",
      "Epoch 13/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8456 - loss: 0.3621 - val_accuracy: 0.8824 - val_loss: 0.3984\n",
      "Epoch 14/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8591 - loss: 0.3518 - val_accuracy: 0.8824 - val_loss: 0.3927\n",
      "Epoch 15/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8792 - loss: 0.3416 - val_accuracy: 0.8824 - val_loss: 0.3890\n",
      "Epoch 16/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8725 - loss: 0.3324 - val_accuracy: 0.8824 - val_loss: 0.3834\n",
      "Epoch 17/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8792 - loss: 0.3240 - val_accuracy: 0.8824 - val_loss: 0.3796\n",
      "Epoch 18/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8859 - loss: 0.3153 - val_accuracy: 0.8824 - val_loss: 0.3751\n",
      "Epoch 19/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8926 - loss: 0.3076 - val_accuracy: 0.8824 - val_loss: 0.3717\n",
      "Epoch 20/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8792 - loss: 0.2994 - val_accuracy: 0.8824 - val_loss: 0.3683\n",
      "Epoch 21/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8926 - loss: 0.2911 - val_accuracy: 0.8824 - val_loss: 0.3655\n",
      "Epoch 22/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8926 - loss: 0.2850 - val_accuracy: 0.8824 - val_loss: 0.3641\n",
      "Epoch 23/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8993 - loss: 0.2774 - val_accuracy: 0.8824 - val_loss: 0.3604\n",
      "Epoch 24/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8926 - loss: 0.2711 - val_accuracy: 0.9412 - val_loss: 0.3578\n",
      "Epoch 25/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8993 - loss: 0.2635 - val_accuracy: 0.9412 - val_loss: 0.3570\n",
      "Epoch 26/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8993 - loss: 0.2568 - val_accuracy: 0.9412 - val_loss: 0.3545\n",
      "Epoch 27/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8993 - loss: 0.2518 - val_accuracy: 0.9412 - val_loss: 0.3534\n",
      "Epoch 28/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8993 - loss: 0.2462 - val_accuracy: 0.9412 - val_loss: 0.3523\n",
      "Epoch 29/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9060 - loss: 0.2387 - val_accuracy: 0.9412 - val_loss: 0.3515\n",
      "Epoch 30/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9128 - loss: 0.2324 - val_accuracy: 0.9412 - val_loss: 0.3529\n",
      "Epoch 31/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9060 - loss: 0.2271 - val_accuracy: 0.9412 - val_loss: 0.3496\n",
      "Epoch 32/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9060 - loss: 0.2207 - val_accuracy: 0.9412 - val_loss: 0.3486\n",
      "Epoch 33/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9329 - loss: 0.2161 - val_accuracy: 0.9412 - val_loss: 0.3489\n",
      "Epoch 34/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9396 - loss: 0.2098 - val_accuracy: 0.9412 - val_loss: 0.3483\n",
      "Epoch 35/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9329 - loss: 0.2052 - val_accuracy: 0.9412 - val_loss: 0.3480\n",
      "Epoch 36/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9396 - loss: 0.1995 - val_accuracy: 0.9412 - val_loss: 0.3499\n",
      "Epoch 37/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9463 - loss: 0.1954 - val_accuracy: 0.9412 - val_loss: 0.3485\n",
      "Epoch 38/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9463 - loss: 0.1906 - val_accuracy: 0.9412 - val_loss: 0.3469\n",
      "Epoch 39/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9597 - loss: 0.1849 - val_accuracy: 0.9412 - val_loss: 0.3486\n",
      "Epoch 40/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9530 - loss: 0.1797 - val_accuracy: 0.9412 - val_loss: 0.3480\n",
      "Epoch 41/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9597 - loss: 0.1758 - val_accuracy: 0.8824 - val_loss: 0.3515\n",
      "Epoch 42/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9664 - loss: 0.1714 - val_accuracy: 0.8824 - val_loss: 0.3496\n",
      "Epoch 43/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9799 - loss: 0.1676 - val_accuracy: 0.8824 - val_loss: 0.3489\n",
      "Epoch 44/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9732 - loss: 0.1638 - val_accuracy: 0.8824 - val_loss: 0.3498\n",
      "Epoch 45/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9799 - loss: 0.1590 - val_accuracy: 0.8824 - val_loss: 0.3489\n",
      "Epoch 46/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9799 - loss: 0.1556 - val_accuracy: 0.8824 - val_loss: 0.3502\n",
      "Epoch 47/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9799 - loss: 0.1512 - val_accuracy: 0.8824 - val_loss: 0.3519\n",
      "Epoch 48/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9933 - loss: 0.1470 - val_accuracy: 0.8824 - val_loss: 0.3526\n",
      "Epoch 49/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9866 - loss: 0.1434 - val_accuracy: 0.8824 - val_loss: 0.3533\n",
      "Epoch 50/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9933 - loss: 0.1398 - val_accuracy: 0.8824 - val_loss: 0.3543\n",
      "Epoch 51/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9933 - loss: 0.1364 - val_accuracy: 0.8824 - val_loss: 0.3550\n",
      "Epoch 52/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9933 - loss: 0.1328 - val_accuracy: 0.8824 - val_loss: 0.3545\n",
      "Epoch 53/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1296 - val_accuracy: 0.8824 - val_loss: 0.3540\n",
      "Epoch 54/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1270 - val_accuracy: 0.8824 - val_loss: 0.3530\n",
      "Epoch 55/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1233 - val_accuracy: 0.8824 - val_loss: 0.3542\n",
      "Epoch 56/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.1195 - val_accuracy: 0.8824 - val_loss: 0.3552\n",
      "Epoch 57/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.1168 - val_accuracy: 0.8824 - val_loss: 0.3574\n",
      "Epoch 58/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.1142 - val_accuracy: 0.8824 - val_loss: 0.3578\n",
      "Epoch 59/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1116 - val_accuracy: 0.8824 - val_loss: 0.3605\n",
      "Epoch 60/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1087 - val_accuracy: 0.8824 - val_loss: 0.3606\n",
      "Epoch 61/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1061 - val_accuracy: 0.8824 - val_loss: 0.3624\n",
      "Epoch 62/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1035 - val_accuracy: 0.8824 - val_loss: 0.3644\n",
      "Epoch 63/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.1004 - val_accuracy: 0.8824 - val_loss: 0.3617\n",
      "Epoch 64/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0983 - val_accuracy: 0.8824 - val_loss: 0.3636\n",
      "Epoch 65/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0957 - val_accuracy: 0.8824 - val_loss: 0.3662\n",
      "Epoch 66/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0928 - val_accuracy: 0.8824 - val_loss: 0.3681\n",
      "Epoch 67/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0909 - val_accuracy: 0.8824 - val_loss: 0.3692\n",
      "Epoch 68/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0887 - val_accuracy: 0.8824 - val_loss: 0.3688\n",
      "Epoch 69/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0866 - val_accuracy: 0.8824 - val_loss: 0.3707\n",
      "Epoch 70/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0851 - val_accuracy: 0.8824 - val_loss: 0.3702\n",
      "Epoch 71/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0826 - val_accuracy: 0.8824 - val_loss: 0.3705\n",
      "Epoch 72/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0807 - val_accuracy: 0.8824 - val_loss: 0.3708\n",
      "Epoch 73/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0785 - val_accuracy: 0.8824 - val_loss: 0.3733\n",
      "Epoch 74/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0767 - val_accuracy: 0.8824 - val_loss: 0.3734\n",
      "Epoch 75/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0756 - val_accuracy: 0.8824 - val_loss: 0.3754\n",
      "Epoch 76/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0735 - val_accuracy: 0.8824 - val_loss: 0.3766\n",
      "Epoch 77/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0719 - val_accuracy: 0.8824 - val_loss: 0.3794\n",
      "Epoch 78/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0704 - val_accuracy: 0.8824 - val_loss: 0.3800\n",
      "Epoch 79/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0686 - val_accuracy: 0.8824 - val_loss: 0.3816\n",
      "Epoch 80/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0670 - val_accuracy: 0.8824 - val_loss: 0.3830\n",
      "Epoch 81/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0658 - val_accuracy: 0.8824 - val_loss: 0.3840\n",
      "Epoch 82/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0638 - val_accuracy: 0.8824 - val_loss: 0.3856\n",
      "Epoch 83/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0628 - val_accuracy: 0.8824 - val_loss: 0.3871\n",
      "Epoch 84/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0614 - val_accuracy: 0.8824 - val_loss: 0.3866\n",
      "Epoch 85/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0605 - val_accuracy: 0.8824 - val_loss: 0.3892\n",
      "Epoch 86/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0588 - val_accuracy: 0.8824 - val_loss: 0.3912\n",
      "Epoch 87/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0576 - val_accuracy: 0.8824 - val_loss: 0.3926\n",
      "Epoch 88/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0566 - val_accuracy: 0.8824 - val_loss: 0.3946\n",
      "Epoch 89/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0552 - val_accuracy: 0.8824 - val_loss: 0.3947\n",
      "Epoch 90/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0541 - val_accuracy: 0.8824 - val_loss: 0.3945\n",
      "Epoch 91/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0528 - val_accuracy: 0.8824 - val_loss: 0.3956\n",
      "Epoch 92/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0518 - val_accuracy: 0.8824 - val_loss: 0.3979\n",
      "Epoch 93/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0509 - val_accuracy: 0.8824 - val_loss: 0.3988\n",
      "Epoch 94/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0499 - val_accuracy: 0.8824 - val_loss: 0.4000\n",
      "Epoch 95/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0487 - val_accuracy: 0.8824 - val_loss: 0.4030\n",
      "Epoch 96/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0478 - val_accuracy: 0.8824 - val_loss: 0.4037\n",
      "Epoch 97/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0471 - val_accuracy: 0.8824 - val_loss: 0.4048\n",
      "Epoch 98/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0459 - val_accuracy: 0.8824 - val_loss: 0.4074\n",
      "Epoch 99/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0451 - val_accuracy: 0.8824 - val_loss: 0.4068\n",
      "Epoch 100/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0442 - val_accuracy: 0.8824 - val_loss: 0.4087\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n"
     ]
    }
   ],
   "source": [
    "hist_final = final_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=1, validation_split=0.1)\n",
    "y_prob_final = final_model.predict(X_test).ravel()\n",
    "y_pred_final = (y_prob_final >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d37e3f9b-adce-4ba6-b895-042bf1f3eb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8095238095238095"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b485941-d9a4-4e0d-91c6-3ad602257846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred_final, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd0d8481-e305-47e5-a1f0-10fdbf265954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred_final, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "481ad0f1-2ed9-4782-884d-90fbd4280c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647058823529411"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred_final, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c01b8acc-d189-49fb-af36-11937ea45b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           M       0.75      0.95      0.84        22\\n           R       0.93      0.65      0.76        20\\n\\n    accuracy                           0.81        42\\n   macro avg       0.84      0.80      0.80        42\\nweighted avg       0.84      0.81      0.80        42\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred_final, target_names=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "46f79bcc-e92c-497e-ada1-7829c3080c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21,  1],\n",
       "       [ 7, 13]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "858b70da-f7d0-439c-925d-f0cb8f424720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist_base.history['val_loss'], label='base val_loss')\n",
    "plt.plot(hist_final.history['val_loss'], label='tuned val_loss')\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist_base.history['val_accuracy'], label='base val_acc')\n",
    "plt.plot(hist_final.history['val_accuracy'], label='tuned val_acc')\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ded470be-7fe8-4322-893b-8577171c3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'baseline': {\n",
    "        'params': base_model ,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "    },\n",
    "    'best': {\n",
    "        'params': base_model ,\n",
    "        'cv_f1': best_score,\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred_final),\n",
    "        'test_precision': precision_score(y_test, y_pred_final, zero_division=0),\n",
    "        'test_recall': recall_score(y_test, y_pred_final, zero_division=0),\n",
    "        'test_f1': f1_score(y_test, y_pred_final, zero_division=0)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2204b92f-c2be-4146-8910-3952f2a04300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline': {'params': <Sequential name=sequential, built=True>,\n",
       "  'accuracy': 0.8333333333333334,\n",
       "  'precision': 0.8823529411764706,\n",
       "  'recall': 0.75,\n",
       "  'f1': 0.8108108108108109},\n",
       " 'best': {'params': <Sequential name=sequential, built=True>,\n",
       "  'cv_f1': np.float64(0.8371406371406371),\n",
       "  'test_accuracy': 0.8095238095238095,\n",
       "  'test_precision': 0.9285714285714286,\n",
       "  'test_recall': 0.65,\n",
       "  'test_f1': 0.7647058823529411}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789cd3d-3d78-4ff2-aa61-84bb16f7b3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898a86f-0f2c-477f-a81e-75659d681767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
