{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e2c23a-dc5f-45b2-8e77-90f5fbcc00f0",
   "metadata": {},
   "source": [
    "### ARTIFICIAL NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d614110-8928-4ac0-9ea3-7eeb0ad9c4d1",
   "metadata": {},
   "source": [
    "#### \n",
    "Case Study: SONAR — Detecting Mines vs. Rocks\n",
    "\n",
    "1️.Business Objective\n",
    "\n",
    "Goal:\n",
    "To build an intelligent system that can automatically detect whether an underwater sonar signal is reflected from a metallic mine (potentially dangerous) or a harmless rock.\n",
    "\n",
    "This is vital for:\n",
    "\n",
    "1.Maritime safety: Prevent ships and submarines from colliding with mines.\n",
    "\n",
    "2.Naval defense: Identify and safely remove underwater mines.\n",
    "\n",
    "3.Resource exploration: Distinguish between useful metal structures and natural seabed objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e9e5d-867f-4620-b011-9601df96c5df",
   "metadata": {},
   "source": [
    "### \n",
    "2️.Problem Statement\n",
    "\n",
    "In underwater environments, sonar (sound navigation and ranging) is used to detect objects. However, raw sonar signals can be noisy and difficult for humans to interpret consistently.\n",
    "\n",
    "This dataset:\n",
    "\n",
    "•\tContains 208 sonar returns.\n",
    "o\t111 are from metal cylinders (mines).\n",
    "o\t97 are from rocks.\n",
    "•\tEach sonar return is represented by 60 numeric features, each measuring the energy of the signal in a frequency band.\n",
    "\n",
    "The problem:\n",
    "\n",
    "To train a Deep learning model that can learn the difference in signal patterns and classify new sonar signals as either Mine (M) or Rock (R) — accurately and reliably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01fe309-caeb-4c4c-9b8b-eee514c5ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922848b5-c664-4917-a48d-aa9ed87d277f",
   "metadata": {},
   "source": [
    "#### Dataset: \"sonardataset.csv\"\n",
    "\n",
    "Features (Inputs)\n",
    "•\tThere are 60 numerical variables, each representing the energy in a specific frequency band of the sonar signal.\n",
    "•\tIn the original dataset, they’re just unnamed columns V1, V2, ..., V60 — you can keep it clear and simple:\n",
    "\n",
    "⃣2Target (Output)\n",
    "•\tThe label is a single categorical variable indicating:\n",
    "o\t\"M\" for Mine\n",
    "o\t\"R\" for Rock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391c998d-2cc3-49b3-a613-2fe5ac1e1b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sonardataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be5fd95-095c-4e4b-a4ee-fc7f11d34829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_1</th>\n",
       "      <th>x_2</th>\n",
       "      <th>x_3</th>\n",
       "      <th>x_4</th>\n",
       "      <th>x_5</th>\n",
       "      <th>x_6</th>\n",
       "      <th>x_7</th>\n",
       "      <th>x_8</th>\n",
       "      <th>x_9</th>\n",
       "      <th>x_10</th>\n",
       "      <th>...</th>\n",
       "      <th>x_52</th>\n",
       "      <th>x_53</th>\n",
       "      <th>x_54</th>\n",
       "      <th>x_55</th>\n",
       "      <th>x_56</th>\n",
       "      <th>x_57</th>\n",
       "      <th>x_58</th>\n",
       "      <th>x_59</th>\n",
       "      <th>x_60</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      x_1     x_2     x_3     x_4     x_5     x_6     x_7     x_8     x_9  \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "     x_10  ...    x_52    x_53    x_54    x_55    x_56    x_57    x_58  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "     x_59    x_60  Y  \n",
       "0  0.0090  0.0032  R  \n",
       "1  0.0052  0.0044  R  \n",
       "2  0.0095  0.0078  R  \n",
       "3  0.0040  0.0117  R  \n",
       "4  0.0107  0.0094  R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e846d6cd-c62f-4cc6-8e5d-6bd8491b4210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 208 entries, 0 to 207\n",
      "Data columns (total 61 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   x_1     208 non-null    float64\n",
      " 1   x_2     208 non-null    float64\n",
      " 2   x_3     208 non-null    float64\n",
      " 3   x_4     208 non-null    float64\n",
      " 4   x_5     208 non-null    float64\n",
      " 5   x_6     208 non-null    float64\n",
      " 6   x_7     208 non-null    float64\n",
      " 7   x_8     208 non-null    float64\n",
      " 8   x_9     208 non-null    float64\n",
      " 9   x_10    208 non-null    float64\n",
      " 10  x_11    208 non-null    float64\n",
      " 11  x_12    208 non-null    float64\n",
      " 12  x_13    208 non-null    float64\n",
      " 13  x_14    208 non-null    float64\n",
      " 14  x_15    208 non-null    float64\n",
      " 15  x_16    208 non-null    float64\n",
      " 16  x_17    208 non-null    float64\n",
      " 17  x_18    208 non-null    float64\n",
      " 18  x_19    208 non-null    float64\n",
      " 19  x_20    208 non-null    float64\n",
      " 20  x_21    208 non-null    float64\n",
      " 21  x_22    208 non-null    float64\n",
      " 22  x_23    208 non-null    float64\n",
      " 23  x_24    208 non-null    float64\n",
      " 24  x_25    208 non-null    float64\n",
      " 25  x_26    208 non-null    float64\n",
      " 26  x_27    208 non-null    float64\n",
      " 27  x_28    208 non-null    float64\n",
      " 28  x_29    208 non-null    float64\n",
      " 29  x_30    208 non-null    float64\n",
      " 30  x_31    208 non-null    float64\n",
      " 31  x_32    208 non-null    float64\n",
      " 32  x_33    208 non-null    float64\n",
      " 33  x_34    208 non-null    float64\n",
      " 34  x_35    208 non-null    float64\n",
      " 35  x_36    208 non-null    float64\n",
      " 36  x_37    208 non-null    float64\n",
      " 37  x_38    208 non-null    float64\n",
      " 38  x_39    208 non-null    float64\n",
      " 39  x_40    208 non-null    float64\n",
      " 40  x_41    208 non-null    float64\n",
      " 41  x_42    208 non-null    float64\n",
      " 42  x_43    208 non-null    float64\n",
      " 43  x_44    208 non-null    float64\n",
      " 44  x_45    208 non-null    float64\n",
      " 45  x_46    208 non-null    float64\n",
      " 46  x_47    208 non-null    float64\n",
      " 47  x_48    208 non-null    float64\n",
      " 48  x_49    208 non-null    float64\n",
      " 49  x_50    208 non-null    float64\n",
      " 50  x_51    208 non-null    float64\n",
      " 51  x_52    208 non-null    float64\n",
      " 52  x_53    208 non-null    float64\n",
      " 53  x_54    208 non-null    float64\n",
      " 54  x_55    208 non-null    float64\n",
      " 55  x_56    208 non-null    float64\n",
      " 56  x_57    208 non-null    float64\n",
      " 57  x_58    208 non-null    float64\n",
      " 58  x_59    208 non-null    float64\n",
      " 59  x_60    208 non-null    float64\n",
      " 60  Y       208 non-null    object \n",
      "dtypes: float64(60), object(1)\n",
      "memory usage: 99.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9476d221-7a2f-42c8-b945-e3dd903ded5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y\n",
      "M    111\n",
      "R     97\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if df.shape[1] >= 2:\n",
    "    labels = df.iloc[:, -1]\n",
    "    print(labels.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a54837-c018-4a67-960f-7f4d78251dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 60)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric = df.select_dtypes(include=[np.number])\n",
    "numeric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7efdf1dc-5c04-4bce-8158-036af1b8201e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x_1</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.013350</td>\n",
       "      <td>0.02280</td>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.1371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_2</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.032960</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.016450</td>\n",
       "      <td>0.03080</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.2339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_3</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.043832</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.03430</td>\n",
       "      <td>0.057950</td>\n",
       "      <td>0.3059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_4</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.053892</td>\n",
       "      <td>0.046528</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.024375</td>\n",
       "      <td>0.04405</td>\n",
       "      <td>0.064500</td>\n",
       "      <td>0.4264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_5</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.075202</td>\n",
       "      <td>0.055552</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.038050</td>\n",
       "      <td>0.06250</td>\n",
       "      <td>0.100275</td>\n",
       "      <td>0.4010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_6</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.104570</td>\n",
       "      <td>0.059105</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.067025</td>\n",
       "      <td>0.09215</td>\n",
       "      <td>0.134125</td>\n",
       "      <td>0.3823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_7</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.121747</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.10695</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.3729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_8</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.134799</td>\n",
       "      <td>0.085152</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.080425</td>\n",
       "      <td>0.11210</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.4590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_9</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.178003</td>\n",
       "      <td>0.118387</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.097025</td>\n",
       "      <td>0.15225</td>\n",
       "      <td>0.233425</td>\n",
       "      <td>0.6828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_10</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.208259</td>\n",
       "      <td>0.134416</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.111275</td>\n",
       "      <td>0.18240</td>\n",
       "      <td>0.268700</td>\n",
       "      <td>0.7106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_11</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.236013</td>\n",
       "      <td>0.132705</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.22480</td>\n",
       "      <td>0.301650</td>\n",
       "      <td>0.7342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_12</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.250221</td>\n",
       "      <td>0.140072</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.24905</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.7060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_13</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.273305</td>\n",
       "      <td>0.140962</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.166125</td>\n",
       "      <td>0.26395</td>\n",
       "      <td>0.351250</td>\n",
       "      <td>0.7131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_14</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.296568</td>\n",
       "      <td>0.164474</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.28110</td>\n",
       "      <td>0.386175</td>\n",
       "      <td>0.9970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_15</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.320201</td>\n",
       "      <td>0.205427</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.164625</td>\n",
       "      <td>0.28170</td>\n",
       "      <td>0.452925</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_16</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.378487</td>\n",
       "      <td>0.232650</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.196300</td>\n",
       "      <td>0.30470</td>\n",
       "      <td>0.535725</td>\n",
       "      <td>0.9988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_17</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.415983</td>\n",
       "      <td>0.263677</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.205850</td>\n",
       "      <td>0.30840</td>\n",
       "      <td>0.659425</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_18</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.452318</td>\n",
       "      <td>0.261529</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.242075</td>\n",
       "      <td>0.36830</td>\n",
       "      <td>0.679050</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_19</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.504812</td>\n",
       "      <td>0.257988</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.299075</td>\n",
       "      <td>0.43495</td>\n",
       "      <td>0.731400</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_20</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.563047</td>\n",
       "      <td>0.262653</td>\n",
       "      <td>0.0656</td>\n",
       "      <td>0.350625</td>\n",
       "      <td>0.54250</td>\n",
       "      <td>0.809325</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_21</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.609060</td>\n",
       "      <td>0.257818</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.399725</td>\n",
       "      <td>0.61770</td>\n",
       "      <td>0.816975</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_22</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.624275</td>\n",
       "      <td>0.255883</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.406925</td>\n",
       "      <td>0.66490</td>\n",
       "      <td>0.831975</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_23</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.646975</td>\n",
       "      <td>0.250175</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>0.450225</td>\n",
       "      <td>0.69970</td>\n",
       "      <td>0.848575</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_24</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.239116</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.540725</td>\n",
       "      <td>0.69850</td>\n",
       "      <td>0.872175</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_25</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.675424</td>\n",
       "      <td>0.244926</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.525800</td>\n",
       "      <td>0.72110</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_26</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.699866</td>\n",
       "      <td>0.237228</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.544175</td>\n",
       "      <td>0.75450</td>\n",
       "      <td>0.893800</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_27</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.702155</td>\n",
       "      <td>0.245657</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.531900</td>\n",
       "      <td>0.74560</td>\n",
       "      <td>0.917100</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_28</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.694024</td>\n",
       "      <td>0.237189</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.534775</td>\n",
       "      <td>0.73190</td>\n",
       "      <td>0.900275</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_29</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.642074</td>\n",
       "      <td>0.240250</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.68080</td>\n",
       "      <td>0.852125</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_30</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.580928</td>\n",
       "      <td>0.220749</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.60715</td>\n",
       "      <td>0.735175</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_31</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.504475</td>\n",
       "      <td>0.213992</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.345550</td>\n",
       "      <td>0.49035</td>\n",
       "      <td>0.641950</td>\n",
       "      <td>0.9657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_32</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.439040</td>\n",
       "      <td>0.213237</td>\n",
       "      <td>0.0404</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.42960</td>\n",
       "      <td>0.580300</td>\n",
       "      <td>0.9306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_33</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.417220</td>\n",
       "      <td>0.206513</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.257875</td>\n",
       "      <td>0.39120</td>\n",
       "      <td>0.556125</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_34</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.403233</td>\n",
       "      <td>0.231242</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.217575</td>\n",
       "      <td>0.35105</td>\n",
       "      <td>0.596125</td>\n",
       "      <td>0.9647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_35</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.392571</td>\n",
       "      <td>0.259132</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.179375</td>\n",
       "      <td>0.31275</td>\n",
       "      <td>0.593350</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_36</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.384848</td>\n",
       "      <td>0.264121</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.154350</td>\n",
       "      <td>0.32115</td>\n",
       "      <td>0.556525</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_37</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.363807</td>\n",
       "      <td>0.239912</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.160100</td>\n",
       "      <td>0.30630</td>\n",
       "      <td>0.518900</td>\n",
       "      <td>0.9497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_38</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.339657</td>\n",
       "      <td>0.212973</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.174275</td>\n",
       "      <td>0.31270</td>\n",
       "      <td>0.440550</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_39</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.199075</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.173975</td>\n",
       "      <td>0.28350</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.9857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_40</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.311207</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.186450</td>\n",
       "      <td>0.27805</td>\n",
       "      <td>0.424350</td>\n",
       "      <td>0.9297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_41</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.289252</td>\n",
       "      <td>0.171111</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.163100</td>\n",
       "      <td>0.25950</td>\n",
       "      <td>0.387525</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_42</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.278293</td>\n",
       "      <td>0.168728</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.158900</td>\n",
       "      <td>0.24510</td>\n",
       "      <td>0.384250</td>\n",
       "      <td>0.8246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_43</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.246542</td>\n",
       "      <td>0.138993</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.155200</td>\n",
       "      <td>0.22255</td>\n",
       "      <td>0.324525</td>\n",
       "      <td>0.7733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_44</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.214075</td>\n",
       "      <td>0.133291</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.126875</td>\n",
       "      <td>0.17770</td>\n",
       "      <td>0.271750</td>\n",
       "      <td>0.7762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_45</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.197232</td>\n",
       "      <td>0.151628</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.094475</td>\n",
       "      <td>0.14800</td>\n",
       "      <td>0.231550</td>\n",
       "      <td>0.7034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_46</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.160631</td>\n",
       "      <td>0.133938</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.068550</td>\n",
       "      <td>0.12135</td>\n",
       "      <td>0.200375</td>\n",
       "      <td>0.7292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_47</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.122453</td>\n",
       "      <td>0.086953</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.064250</td>\n",
       "      <td>0.10165</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>0.5522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_48</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.091424</td>\n",
       "      <td>0.062417</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.045125</td>\n",
       "      <td>0.07810</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.3339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_49</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.051929</td>\n",
       "      <td>0.035954</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.026350</td>\n",
       "      <td>0.04470</td>\n",
       "      <td>0.068525</td>\n",
       "      <td>0.1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_50</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.020424</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.011550</td>\n",
       "      <td>0.01790</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.0825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_51</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.012008</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.008425</td>\n",
       "      <td>0.01390</td>\n",
       "      <td>0.020825</td>\n",
       "      <td>0.1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_52</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.007275</td>\n",
       "      <td>0.01140</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>0.0709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_53</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.010709</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.00955</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.0390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_54</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.010941</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.005375</td>\n",
       "      <td>0.00930</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.0352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_55</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.009290</td>\n",
       "      <td>0.007088</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.0447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_56</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.008222</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.00685</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>0.0394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_57</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.00595</td>\n",
       "      <td>0.010425</td>\n",
       "      <td>0.0355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_58</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.007949</td>\n",
       "      <td>0.006470</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.00580</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.0440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_59</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.007941</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.003675</td>\n",
       "      <td>0.00640</td>\n",
       "      <td>0.010325</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x_60</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.006507</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.00530</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>0.0439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count      mean       std     min       25%      50%       75%     max\n",
       "x_1   208.0  0.029164  0.022991  0.0015  0.013350  0.02280  0.035550  0.1371\n",
       "x_2   208.0  0.038437  0.032960  0.0006  0.016450  0.03080  0.047950  0.2339\n",
       "x_3   208.0  0.043832  0.038428  0.0015  0.018950  0.03430  0.057950  0.3059\n",
       "x_4   208.0  0.053892  0.046528  0.0058  0.024375  0.04405  0.064500  0.4264\n",
       "x_5   208.0  0.075202  0.055552  0.0067  0.038050  0.06250  0.100275  0.4010\n",
       "x_6   208.0  0.104570  0.059105  0.0102  0.067025  0.09215  0.134125  0.3823\n",
       "x_7   208.0  0.121747  0.061788  0.0033  0.080900  0.10695  0.154000  0.3729\n",
       "x_8   208.0  0.134799  0.085152  0.0055  0.080425  0.11210  0.169600  0.4590\n",
       "x_9   208.0  0.178003  0.118387  0.0075  0.097025  0.15225  0.233425  0.6828\n",
       "x_10  208.0  0.208259  0.134416  0.0113  0.111275  0.18240  0.268700  0.7106\n",
       "x_11  208.0  0.236013  0.132705  0.0289  0.129250  0.22480  0.301650  0.7342\n",
       "x_12  208.0  0.250221  0.140072  0.0236  0.133475  0.24905  0.331250  0.7060\n",
       "x_13  208.0  0.273305  0.140962  0.0184  0.166125  0.26395  0.351250  0.7131\n",
       "x_14  208.0  0.296568  0.164474  0.0273  0.175175  0.28110  0.386175  0.9970\n",
       "x_15  208.0  0.320201  0.205427  0.0031  0.164625  0.28170  0.452925  1.0000\n",
       "x_16  208.0  0.378487  0.232650  0.0162  0.196300  0.30470  0.535725  0.9988\n",
       "x_17  208.0  0.415983  0.263677  0.0349  0.205850  0.30840  0.659425  1.0000\n",
       "x_18  208.0  0.452318  0.261529  0.0375  0.242075  0.36830  0.679050  1.0000\n",
       "x_19  208.0  0.504812  0.257988  0.0494  0.299075  0.43495  0.731400  1.0000\n",
       "x_20  208.0  0.563047  0.262653  0.0656  0.350625  0.54250  0.809325  1.0000\n",
       "x_21  208.0  0.609060  0.257818  0.0512  0.399725  0.61770  0.816975  1.0000\n",
       "x_22  208.0  0.624275  0.255883  0.0219  0.406925  0.66490  0.831975  1.0000\n",
       "x_23  208.0  0.646975  0.250175  0.0563  0.450225  0.69970  0.848575  1.0000\n",
       "x_24  208.0  0.672654  0.239116  0.0239  0.540725  0.69850  0.872175  1.0000\n",
       "x_25  208.0  0.675424  0.244926  0.0240  0.525800  0.72110  0.873725  1.0000\n",
       "x_26  208.0  0.699866  0.237228  0.0921  0.544175  0.75450  0.893800  1.0000\n",
       "x_27  208.0  0.702155  0.245657  0.0481  0.531900  0.74560  0.917100  1.0000\n",
       "x_28  208.0  0.694024  0.237189  0.0284  0.534775  0.73190  0.900275  1.0000\n",
       "x_29  208.0  0.642074  0.240250  0.0144  0.463700  0.68080  0.852125  1.0000\n",
       "x_30  208.0  0.580928  0.220749  0.0613  0.411400  0.60715  0.735175  1.0000\n",
       "x_31  208.0  0.504475  0.213992  0.0482  0.345550  0.49035  0.641950  0.9657\n",
       "x_32  208.0  0.439040  0.213237  0.0404  0.281400  0.42960  0.580300  0.9306\n",
       "x_33  208.0  0.417220  0.206513  0.0477  0.257875  0.39120  0.556125  1.0000\n",
       "x_34  208.0  0.403233  0.231242  0.0212  0.217575  0.35105  0.596125  0.9647\n",
       "x_35  208.0  0.392571  0.259132  0.0223  0.179375  0.31275  0.593350  1.0000\n",
       "x_36  208.0  0.384848  0.264121  0.0080  0.154350  0.32115  0.556525  1.0000\n",
       "x_37  208.0  0.363807  0.239912  0.0351  0.160100  0.30630  0.518900  0.9497\n",
       "x_38  208.0  0.339657  0.212973  0.0383  0.174275  0.31270  0.440550  1.0000\n",
       "x_39  208.0  0.325800  0.199075  0.0371  0.173975  0.28350  0.434900  0.9857\n",
       "x_40  208.0  0.311207  0.178662  0.0117  0.186450  0.27805  0.424350  0.9297\n",
       "x_41  208.0  0.289252  0.171111  0.0360  0.163100  0.25950  0.387525  0.8995\n",
       "x_42  208.0  0.278293  0.168728  0.0056  0.158900  0.24510  0.384250  0.8246\n",
       "x_43  208.0  0.246542  0.138993  0.0000  0.155200  0.22255  0.324525  0.7733\n",
       "x_44  208.0  0.214075  0.133291  0.0000  0.126875  0.17770  0.271750  0.7762\n",
       "x_45  208.0  0.197232  0.151628  0.0000  0.094475  0.14800  0.231550  0.7034\n",
       "x_46  208.0  0.160631  0.133938  0.0000  0.068550  0.12135  0.200375  0.7292\n",
       "x_47  208.0  0.122453  0.086953  0.0000  0.064250  0.10165  0.154425  0.5522\n",
       "x_48  208.0  0.091424  0.062417  0.0000  0.045125  0.07810  0.120100  0.3339\n",
       "x_49  208.0  0.051929  0.035954  0.0000  0.026350  0.04470  0.068525  0.1981\n",
       "x_50  208.0  0.020424  0.013665  0.0000  0.011550  0.01790  0.025275  0.0825\n",
       "x_51  208.0  0.016069  0.012008  0.0000  0.008425  0.01390  0.020825  0.1004\n",
       "x_52  208.0  0.013420  0.009634  0.0008  0.007275  0.01140  0.016725  0.0709\n",
       "x_53  208.0  0.010709  0.007060  0.0005  0.005075  0.00955  0.014900  0.0390\n",
       "x_54  208.0  0.010941  0.007301  0.0010  0.005375  0.00930  0.014500  0.0352\n",
       "x_55  208.0  0.009290  0.007088  0.0006  0.004150  0.00750  0.012100  0.0447\n",
       "x_56  208.0  0.008222  0.005736  0.0004  0.004400  0.00685  0.010575  0.0394\n",
       "x_57  208.0  0.007820  0.005785  0.0003  0.003700  0.00595  0.010425  0.0355\n",
       "x_58  208.0  0.007949  0.006470  0.0003  0.003600  0.00580  0.010350  0.0440\n",
       "x_59  208.0  0.007941  0.006181  0.0001  0.003675  0.00640  0.010325  0.0364\n",
       "x_60  208.0  0.006507  0.005031  0.0006  0.003100  0.00530  0.008525  0.0439"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d680a27a-fa54-4019-84f2-c147de5623f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217083d7-e9e1-4dd4-9bfe-4122889b7466",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values.astype(float)  \n",
    "y_raw = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218d6e6-ef3d-4f71-bfd5-46c8f0125ee1",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "1. Data Exploration and Preprocessing\n",
    "   \n",
    "●\tBegin by loading and exploring the \"Alphabets_data.csv\" dataset. Summarize its key features such as the number of samples, features, and classes.\n",
    "●\tExecute necessary data preprocessing steps including data normalization, managing missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950a7af6-19c5-473e-b1f1-12d93d4d9a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8064f0f2-91d4-47b4-a237-8c90f2eb18d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M': np.int64(0), 'R': np.int64(1)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(le.classes_, le.transform(le.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fee8c407-a639-4c01-bd1e-0e0af8c5dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce1b174d-fe1a-47fc-996e-92e2dc9e5ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((166, 60), (42, 60), (166,), (42,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259966cc-4c80-46d1-b50e-032db9fb88d4",
   "metadata": {},
   "source": [
    "#### 2. Model Implementation\n",
    "\n",
    "●\tConstruct a basic ANN model using your chosen high-level neural network library. Ensure your model includes at least one hidden layer.\n",
    "●\tDivide the dataset into training and test sets.\n",
    "●\tTrain your model on the training set and then use it to make predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c3971e3-9c96-4dc4-9776-545f8db1c141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, hidden_layers=(32,), activation='relu', dropout=0.0, lr=1e-3, optimizer_name='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers[0], input_dim=input_dim, activation=activation))\n",
    "    if dropout>0:\n",
    "        model.add(Dropout(dropout))\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation=activation))\n",
    "        if dropout>0:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    if optimizer_name == 'adam':\n",
    "        opt = Adam(learning_rate=lr)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaf2bae4-9b4d-4405-828b-795ff514d670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline model: {'hidden_layers': (32,), 'activation': 'relu', 'dropout': 0.0, 'lr': 0.001, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 16}\n",
      "Epoch 1/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.5034 - loss: 0.7300 - val_accuracy: 0.6471 - val_loss: 0.5926\n",
      "Epoch 2/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5772 - loss: 0.6283 - val_accuracy: 0.7059 - val_loss: 0.5254\n",
      "Epoch 3/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6779 - loss: 0.5585 - val_accuracy: 0.8235 - val_loss: 0.4772\n",
      "Epoch 4/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.7651 - loss: 0.5088 - val_accuracy: 0.8824 - val_loss: 0.4414\n",
      "Epoch 5/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.7785 - loss: 0.4685 - val_accuracy: 0.9412 - val_loss: 0.4149\n",
      "Epoch 6/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8121 - loss: 0.4357 - val_accuracy: 0.9412 - val_loss: 0.3927\n",
      "Epoch 7/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8255 - loss: 0.4070 - val_accuracy: 0.9412 - val_loss: 0.3747\n",
      "Epoch 8/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8456 - loss: 0.3833 - val_accuracy: 0.9412 - val_loss: 0.3620\n",
      "Epoch 9/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8725 - loss: 0.3619 - val_accuracy: 0.9412 - val_loss: 0.3439\n",
      "Epoch 10/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8792 - loss: 0.3417 - val_accuracy: 0.9412 - val_loss: 0.3302\n",
      "Epoch 11/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8859 - loss: 0.3242 - val_accuracy: 1.0000 - val_loss: 0.3198\n",
      "Epoch 12/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8993 - loss: 0.3078 - val_accuracy: 1.0000 - val_loss: 0.3067\n",
      "Epoch 13/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.8926 - loss: 0.2932 - val_accuracy: 0.9412 - val_loss: 0.2946\n",
      "Epoch 14/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9128 - loss: 0.2782 - val_accuracy: 0.9412 - val_loss: 0.2839\n",
      "Epoch 15/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9396 - loss: 0.2653 - val_accuracy: 0.9412 - val_loss: 0.2781\n",
      "Epoch 16/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9463 - loss: 0.2526 - val_accuracy: 0.9412 - val_loss: 0.2675\n",
      "Epoch 17/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9597 - loss: 0.2404 - val_accuracy: 0.9412 - val_loss: 0.2615\n",
      "Epoch 18/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9597 - loss: 0.2306 - val_accuracy: 0.9412 - val_loss: 0.2538\n",
      "Epoch 19/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9732 - loss: 0.2192 - val_accuracy: 0.9412 - val_loss: 0.2476\n",
      "Epoch 20/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9799 - loss: 0.2100 - val_accuracy: 0.9412 - val_loss: 0.2402\n",
      "Epoch 21/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9799 - loss: 0.2007 - val_accuracy: 0.8824 - val_loss: 0.2351\n",
      "Epoch 22/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.9799 - loss: 0.1914 - val_accuracy: 0.9412 - val_loss: 0.2289\n",
      "Epoch 23/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9866 - loss: 0.1830 - val_accuracy: 0.9412 - val_loss: 0.2246\n",
      "Epoch 24/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9933 - loss: 0.1758 - val_accuracy: 0.9412 - val_loss: 0.2210\n",
      "Epoch 25/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9933 - loss: 0.1677 - val_accuracy: 0.9412 - val_loss: 0.2200\n",
      "Epoch 26/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9933 - loss: 0.1614 - val_accuracy: 0.9412 - val_loss: 0.2200\n",
      "Epoch 27/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9933 - loss: 0.1544 - val_accuracy: 0.9412 - val_loss: 0.2154\n",
      "Epoch 28/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1482 - val_accuracy: 0.9412 - val_loss: 0.2099\n",
      "Epoch 29/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9933 - loss: 0.1421 - val_accuracy: 0.9412 - val_loss: 0.2086\n",
      "Epoch 30/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1356 - val_accuracy: 0.9412 - val_loss: 0.2071\n",
      "Epoch 31/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1303 - val_accuracy: 0.9412 - val_loss: 0.2053\n",
      "Epoch 32/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9933 - loss: 0.1255 - val_accuracy: 0.9412 - val_loss: 0.2031\n",
      "Epoch 33/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1207 - val_accuracy: 0.9412 - val_loss: 0.2031\n",
      "Epoch 34/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9933 - loss: 0.1155 - val_accuracy: 0.9412 - val_loss: 0.2004\n",
      "Epoch 35/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.9933 - loss: 0.1117 - val_accuracy: 0.9412 - val_loss: 0.1981\n",
      "Epoch 36/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.1063 - val_accuracy: 0.9412 - val_loss: 0.2022\n",
      "Epoch 37/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 1.0000 - loss: 0.1027 - val_accuracy: 0.9412 - val_loss: 0.2032\n",
      "Epoch 38/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0985 - val_accuracy: 0.9412 - val_loss: 0.2021\n",
      "Epoch 39/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0948 - val_accuracy: 0.9412 - val_loss: 0.1992\n",
      "Epoch 40/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0910 - val_accuracy: 0.9412 - val_loss: 0.1992\n",
      "Epoch 41/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0875 - val_accuracy: 0.9412 - val_loss: 0.1978\n",
      "Epoch 42/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0851 - val_accuracy: 0.9412 - val_loss: 0.2000\n",
      "Epoch 43/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0813 - val_accuracy: 0.9412 - val_loss: 0.1993\n",
      "Epoch 44/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0781 - val_accuracy: 0.9412 - val_loss: 0.1991\n",
      "Epoch 45/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 1.0000 - loss: 0.0755 - val_accuracy: 0.9412 - val_loss: 0.1997\n",
      "Epoch 46/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 1.0000 - loss: 0.0725 - val_accuracy: 0.9412 - val_loss: 0.1988\n",
      "Epoch 47/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0702 - val_accuracy: 0.9412 - val_loss: 0.1994\n",
      "Epoch 48/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 1.0000 - loss: 0.0673 - val_accuracy: 0.9412 - val_loss: 0.1991\n",
      "Epoch 49/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 1.0000 - loss: 0.0649 - val_accuracy: 0.9412 - val_loss: 0.2010\n",
      "Epoch 50/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.0627 - val_accuracy: 0.9412 - val_loss: 0.1999\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n"
     ]
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "base_params = dict(hidden_layers=(32,), activation='relu', dropout=0.0, lr=1e-3, optimizer_name='adam', epochs=50, batch_size=16)\n",
    "print(\"Training baseline model:\", base_params)\n",
    "base_model = build_model(input_dim, hidden_layers=base_params['hidden_layers'],\n",
    "                         activation=base_params['activation'], dropout=base_params['dropout'],\n",
    "                         lr=base_params['lr'], optimizer_name=base_params['optimizer_name'])\n",
    "hist_base = base_model.fit(X_train, y_train, validation_split=0.1, epochs=base_params['epochs'],\n",
    "                           batch_size=base_params['batch_size'], verbose=1)\n",
    "y_prob = base_model.predict(X_test).ravel()\n",
    "y_pred = (y_prob >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5584dcd-f2a8-4c28-8c9f-8fc0405632f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccaf953a-60d7-4bb0-be5e-d1aaa5a82182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8823529411764706"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc8bb20a-75c7-46ed-8d5e-a8e983540e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7da84c91-5181-4353-8a57-bcc1c1891e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8108108108108109"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "490d316c-3854-4adf-905c-6fead9b266ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           M       0.80      0.91      0.85        22\\n           R       0.88      0.75      0.81        20\\n\\n    accuracy                           0.83        42\\n   macro avg       0.84      0.83      0.83        42\\nweighted avg       0.84      0.83      0.83        42\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, y_pred, target_names=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "173a0a9e-3e1f-4348-8877-1f42fa756c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  2],\n",
       "       [ 5, 15]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb30e5-b3d3-4d6f-92c4-1f4fd21a3607",
   "metadata": {},
   "source": [
    "#### 3. Hyperparameter Tuning\n",
    "\n",
    "●\tModify various hyperparameters, such as the number of hidden layers, neurons per hidden layer, activation functions, and learning rate, to observe their impact on model performance.\n",
    "●\tAdopt a structured approach like grid search or random search for hyperparameter tuning, documenting your methodology thoroughly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afc5b27a-cada-48ae-938b-93428a8504cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_layers': [(16,), (32,), (64,), (32,32), (64,32)],\n",
    "    'activation': ['relu','tanh'],\n",
    "    'dropout': [0.0, 0.1, 0.2],\n",
    "    'lr': [1e-2, 1e-3, 5e-4],\n",
    "    'optimizer_name': ['adam','sgd'],\n",
    "    'epochs': [50, 100],\n",
    "    'batch_size': [8, 16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17cd41d3-b06c-453b-a471-7f7abef88195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "all_combinations = list(product(*(param_grid[k] for k in param_grid)))\n",
    "len(all_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9be784d-91cc-4895-9930-7d03e6818d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "n_iter = 12\n",
    "random.seed(42)\n",
    "sampled = random.sample(all_combinations, min(n_iter, len(all_combinations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f27cebcd-e63c-4bc7-b489-2586a67473d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64dac29a-268f-44d5-a97c-002d1d2f4319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8489b610-7a7a-428d-b5a4-681eefbd3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/12] Testing params: {'hidden_layers': (64, 32), 'activation': 'tanh', 'dropout': 0.0, 'lr': 0.01, 'optimizer_name': 'sgd', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DF49023560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DF49023560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      " -> mean CV F1: 0.8371406371406371\n",
      "\n",
      "[2/12] Testing params: {'hidden_layers': (16,), 'activation': 'tanh', 'dropout': 0.1, 'lr': 0.0005, 'optimizer_name': 'adam', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      " -> mean CV F1: 0.7677314945116802\n",
      "\n",
      "[3/12] Testing params: {'hidden_layers': (16,), 'activation': 'relu', 'dropout': 0.1, 'lr': 0.01, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 16}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      " -> mean CV F1: 0.7738359201773837\n",
      "\n",
      "[4/12] Testing params: {'hidden_layers': (32,), 'activation': 'tanh', 'dropout': 0.2, 'lr': 0.0005, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 16}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      " -> mean CV F1: 0.7822871131316664\n",
      "\n",
      "[5/12] Testing params: {'hidden_layers': (32,), 'activation': 'tanh', 'dropout': 0.1, 'lr': 0.001, 'optimizer_name': 'adam', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      " -> mean CV F1: 0.755060959512324\n",
      "\n",
      "[6/12] Testing params: {'hidden_layers': (32,), 'activation': 'tanh', 'dropout': 0.0, 'lr': 0.001, 'optimizer_name': 'sgd', 'epochs': 50, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      " -> mean CV F1: 0.6687653562653563\n",
      "\n",
      "[7/12] Testing params: {'hidden_layers': (16,), 'activation': 'tanh', 'dropout': 0.2, 'lr': 0.0005, 'optimizer_name': 'sgd', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      " -> mean CV F1: 0.702517651430695\n",
      "\n",
      "[8/12] Testing params: {'hidden_layers': (16,), 'activation': 'tanh', 'dropout': 0.1, 'lr': 0.001, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      " -> mean CV F1: 0.7894744483159117\n",
      "\n",
      "[9/12] Testing params: {'hidden_layers': (64, 32), 'activation': 'tanh', 'dropout': 0.1, 'lr': 0.0005, 'optimizer_name': 'sgd', 'epochs': 50, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      " -> mean CV F1: 0.6840853606593822\n",
      "\n",
      "[10/12] Testing params: {'hidden_layers': (32, 32), 'activation': 'tanh', 'dropout': 0.2, 'lr': 0.01, 'optimizer_name': 'sgd', 'epochs': 100, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      " -> mean CV F1: 0.7636922697898307\n",
      "\n",
      "[11/12] Testing params: {'hidden_layers': (16,), 'activation': 'tanh', 'dropout': 0.0, 'lr': 0.0005, 'optimizer_name': 'adam', 'epochs': 50, 'batch_size': 16}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      " -> mean CV F1: 0.7269343089117808\n",
      "\n",
      "[12/12] Testing params: {'hidden_layers': (64, 32), 'activation': 'relu', 'dropout': 0.1, 'lr': 0.01, 'optimizer_name': 'sgd', 'epochs': 50, 'batch_size': 8}\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      " -> mean CV F1: 0.7573593073593073\n",
      "\n",
      "Random search done in 7.11 minutes.\n",
      "Best CV F1: 0.8371406371406371\n",
      "Best params: {'hidden_layers': (64, 32), 'activation': 'tanh', 'dropout': 0.0, 'lr': 0.01, 'optimizer_name': 'sgd', 'epochs': 100, 'batch_size': 8}\n"
     ]
    }
   ],
   "source": [
    "start_all = time.time()\n",
    "best_score = -1.0\n",
    "best_res = None\n",
    "results = []\n",
    "for idx, combo in enumerate(sampled, 1):\n",
    "    params = dict(zip(param_grid.keys(), combo))\n",
    "    print(f\"\\n[{idx}/{len(sampled)}] Testing params: {params}\")\n",
    "    cv_scores = []\n",
    "    # cross-validation loop\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "        model = build_model(input_dim, hidden_layers=params['hidden_layers'],\n",
    "                            activation=params['activation'], dropout=params['dropout'],\n",
    "                            lr=params['lr'], optimizer_name=params['optimizer_name'])\n",
    "        model.fit(X_tr, y_tr, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0)\n",
    "        yv = (model.predict(X_val).ravel() >= 0.5).astype(int)\n",
    "        cv_scores.append(f1_score(y_val, yv, zero_division=0))\n",
    "    mean_cv = np.mean(cv_scores)\n",
    "    results.append((params, mean_cv))\n",
    "    print(\" -> mean CV F1:\", mean_cv)\n",
    "    if mean_cv > best_score:\n",
    "        best_score = mean_cv\n",
    "        best_res = (params, mean_cv)\n",
    "end_all = time.time()\n",
    "print(f\"\\nRandom search done in {(end_all - start_all)/60:.2f} minutes.\")\n",
    "print(\"Best CV F1:\", best_score)\n",
    "print(\"Best params:\", best_res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea60378-cf48-4452-bde3-d5eeaca981f2",
   "metadata": {},
   "source": [
    "### 4. Evaluation\n",
    "●\tEmploy suitable metrics such as accuracy, precision, recall, and F1-score to evaluate your model's performance.\n",
    "●\tDiscuss the performance differences between the model with default hyperparameters and the tuned model, emphasizing the effects of hyperparameter tuning.\n",
    "\n",
    "Evaluation Criteria\n",
    "\n",
    "●\tAccuracy and completeness of the implementation.\n",
    "●\tProficiency in data preprocessing and model development.\n",
    "●\tSystematic approach and thoroughness in hyperparameter tuning.\n",
    "●\tDepth of evaluation and discussion.\n",
    "●\tOverall quality of the report.\n",
    "\n",
    "Additional Resources\n",
    "\n",
    "●\tTensorFlow Documentation\n",
    "●\tKeras Documentation\n",
    "We wish you the best of luck with this assignment. Enjoy exploring the fascinating world of neural networks and the power of hyperparameter tuning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f690131-2071-4553-b863-6daee9db8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_res[0]\n",
    "final_model = build_model(input_dim, hidden_layers=best_params['hidden_layers'],\n",
    "                          activation=best_params['activation'], dropout=best_params['dropout'],\n",
    "                          lr=best_params['lr'], optimizer_name=best_params['optimizer_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa277f7f-f34a-42de-b61a-6c31a312e44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layers': (64, 32),\n",
       " 'activation': 'tanh',\n",
       " 'dropout': 0.0,\n",
       " 'lr': 0.01,\n",
       " 'optimizer_name': 'sgd',\n",
       " 'epochs': 100,\n",
       " 'batch_size': 8}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71846f4-6180-4390-818e-3afa4c5601e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5302 - loss: 0.6931 - val_accuracy: 0.6471 - val_loss: 0.5971\n",
      "Epoch 2/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7114 - loss: 0.5880 - val_accuracy: 0.6471 - val_loss: 0.5405\n",
      "Epoch 3/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7584 - loss: 0.5320 - val_accuracy: 0.7647 - val_loss: 0.5070\n",
      "Epoch 4/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7785 - loss: 0.4973 - val_accuracy: 0.7059 - val_loss: 0.4826\n",
      "Epoch 5/100\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7852 - loss: 0.4724 - val_accuracy: 0.7059 - val_loss: 0.4663\n",
      "Epoch 6/100\n",
      "\u001b[1m 1/19\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.4305"
     ]
    }
   ],
   "source": [
    "hist_final = final_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=1, validation_split=0.1)\n",
    "y_prob_final = final_model.predict(X_test).ravel()\n",
    "y_pred_final = (y_prob_final >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e3f9b-adce-4ba6-b895-042bf1f3eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b485941-d9a4-4e0d-91c6-3ad602257846",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_final, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d8481-e305-47e5-a1f0-10fdbf265954",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_final, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ad0f1-2ed9-4782-884d-90fbd4280c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_final, zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01b8acc-d189-49fb-af36-11937ea45b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test, y_pred_final, target_names=le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f79bcc-e92c-497e-ada1-7829c3080c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b70da-f7d0-439c-925d-f0cb8f424720",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['val_loss'], label='base val_loss')\n",
    "plt.plot(hist_final.history['val_loss'], label='tuned val_loss')\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['val_accuracy'], label='base val_acc')\n",
    "plt.plot(hist_final.history['val_accuracy'], label='tuned val_acc')\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded470be-7fe8-4322-893b-8577171c3b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'baseline': {\n",
    "        'params': base_model ,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "    },\n",
    "    'best': {\n",
    "        'params': base_model ,\n",
    "        'cv_f1': best_score,\n",
    "        'test_accuracy': accuracy_score(y_test, y_pred_final),\n",
    "        'test_precision': precision_score(y_test, y_pred_final, zero_division=0),\n",
    "        'test_recall': recall_score(y_test, y_pred_final, zero_division=0),\n",
    "        'test_f1': f1_score(y_test, y_pred_final, zero_division=0)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2204b92f-c2be-4146-8910-3952f2a04300",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3789cd3d-3d78-4ff2-aa61-84bb16f7b3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898a86f-0f2c-477f-a81e-75659d681767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
